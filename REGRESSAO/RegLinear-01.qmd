---
title: "Regressão Linear"
execute: 
  warning: false
format:
  html:
    toc: true
    html-math-method: katex
    css: styles.css
    df-print: paged
---

```{r}
library(tidyverse) 
library(janitor)
library(nortest)
library(DataExplorer)
library(ggrepel)
library (plotly)
library(MASS)
library(rgl)
library(car)

```

# Base de Dados

Para os próximos exemplos iremos utilizar a base de dados MTCARS que possui informações sobre 32 veículos de revista Motor Trends de 1974.

```{r}
# Selecionar base mtcars e criar uma coluna com os nomes dos modelos
df <- mtcars |> 
  rownames_to_column(var = "name") |> as_tibble()

df
```

# Variável dependente

Para nossos experimentos, iremos selecionar como variável dependente o consumo (mpg). Ou seja, tentaremos criar modelo preditivos que, com base em variáveis explicativas, tentarão prever o valor do consumo (mpg) de um automóvel.

## Visualizando a variável dependente

Apenas para começar, vamos dar uma breve olhada em um histograma da variável dependente.

```{r}
df |> 
  ggplot(aes(x=mpg)) +
  geom_histogram(binwidth = 2) 
```

## Teste de Normalidade Shapiro-Francia

Veja que este **NÃO** é uma premissa para uso da técnica de regressão linear, isso é geralmente na validação dos resíduos conforme veremos mais adiante. Estamos apenas apresentando como fazer o teste e analisando a variável dependente.

```{r}
# Teste de normalidade Shapiro-Francia
# p-valor <= 0.5 é não-normal, ou seja, maior a variável é normal
sf.test(df$mpg)
```

Neste caso, observamos que nossa variável dependente tem uma forma funcional normal.

# Regressão univariada

Como já temos nossa variável dependente o consumo (mpg), iremos agora definir como variável explicativa, a potência (hp). A idéia é tentar criar um modelo preditivo em função desta variável explicativa.

## Correlação

Iremos inicialmente entender como a variável dependente (mpg) está relacionada com a variável explicativa (hp). Por se tratar de duas variáveis quantitativas, iremos utilizar a correlação de **Pearson**. Este é um coeficiente que informa o quão forte é esta correlação, variando de -1 até 1. Sendo: - Negativa (-1) - Quanto **maior** a variável dependente **menor** a variável explicativa - Neutra (0) - Não há correlação entre as variáveis - Positiva (1) - Quanto **maior** a variável dependente **maior** a variável

```{r}
# Variável explicativa escolhida = hp

cor(df$mpg, df$hp)
DataExplorer::plot_correlation(df[c("mpg", "hp")])
```

Nesta caso temos uma considerável correlação negativa (-0.78), ou seja, quanto menor a potência, maior o consumo.

## Gráfico de dispersão:

Através do gráfico abaixo, podemos ter uma intuição visual do que representa a correlação de -0.78. Veja que se traçarmos uma reta atráves dos pontos, quanto maior o valor de mpg, menor o valor de hp e vice versa.

```{r}
df |> 
  ggplot(aes(x = hp, y= mpg)) +
  geom_point() +
  geom_smooth(method = "lm", se = F)+
  geom_text_repel(aes(label = name), size = 2, color = "darkgray")
```

## Criando um modelo linear simples

No caso de uma regressão linear simples, temos a seguinte equação:

Função: $\hat{y} = \alpha + \beta * X1$

O código abaixo, através da função "lm" irá criar um modelo de regressão linear simples, estimando os parâmetros para a equação de nossa reta. Neste caso, iremos utilizar o método "OLS" (Ordinary Least Square) para estimativa dos parâmetros desta equação. Não iremos cobrir os detalhes deste processo, mas há diversos métodos para encontrar estes coeficientes, como Mínimos Quadrados Ordinários (OLS), Máxima Verossimilhança (MLE), entre outros.

```{r}
#Função lm para obter os coeficientes alpha e beta
modelo_uni <- lm(mpg ~ hp, data = df)
modelo_uni
```

Onde, o $\hat{y}$ representa o valor previsto de nosso modelo, o $\alpha$ o intercepto da reta, ou seja, que valor teórico termos caso a variável explicativa fosse zero. Temos também o $\beta$ que é inclinação da reta, ou seja, o quanto da variável explicativa é impactada em uma unidade. $X1$ neste caso, é o valor da nossa variável explicativa (hp).

Olhando estes coeficientes, podemos dizer que a cada 0.07 reduzida no valor da potência, aumentamos em uma milha por galão nosso consumo (mpg=miles per galon).

Neste caso, nossa função ficaria:

$\hat{y} = (30.09886) + [(-0.06823) * X1]$

Ou seja, se quisermos prever o consumo (mpg) à partir apenas da variável explicativa potencia (hp), faríamos:

$(30.09886) + [(-0.06823) * hp]$

Por exemplo, de acordo com nosso modelo, para um veículo com 190 de potência, teremos:

$(30.09886) + [(-0.06823) * 190]$ = $(30.09886) - 12.9637 = \textbf{17.13516}$

Ou seja, nosso modelo prevê um consumo de **17.13** milhas por galão se um veículo tiver **190** de potência.

## Analizando o Modelo

Com o modelo criado anteriormente, podemos juntar nossas estimativas (fitted values) à base de dados originais e comparar nossos resultados:

```{r}
media_mpg = round(summarise(df, m = mean(mpg))[[1]],1)
mpg_previsto = 17.13
hp_previsao = 190

bind_cols(df, modelo_uni$fitted.values) |> 
  rename (fitted = last_col()) |> 
  dplyr::select (orig = mpg, fitted, hp) |>  
  pivot_longer(cols = c("fitted", "orig")) |> 
  dplyr::rename (categoria = name, mpg = value) |> 
  ggplot(aes(x = hp, y = mpg, color = categoria)) +
  geom_point(size = 3, alpha = 0.6) +
  geom_point(x = hp_previsao, y= mpg_previsto, size = 5, color = "red")+
  geom_vline(aes(xintercept = hp_previsao), 
                 linetype = "dashed", color = "darkred")+
  geom_text(aes(hp_previsao, mpg_previsto , label = paste("Previsto: ", mpg_previsto), vjust = -1, hjust = -0.5), color = "red")+
  geom_hline(aes(yintercept = media_mpg), 
                 linetype = "dashed", color = "navyblue")+
  geom_text(aes(270,media_mpg, label = paste("Média: ", media_mpg), vjust = -1), show.legend = FALSE)
  
```

Veja que se tivéssemos escolhido apenas uma **média** da variável dependente (mpg), faríamos uma previsão de consumo de **20.1**. Como utilizamos nosso modelo, nossa previsão mais acurada, prevendo um valor de **17.13**.

Com o modelo criado anteriormente, podemos também utilizar a função **summary**() para extrair algumas informações bem importantes. Vejamos:

```{r}
summary(modelo_uni)
```

Há diversos resultados a serem observados, dentre eles, podemos observar que a estatística F, com p-value menor que 0.05 basicamente nos indica que nosso modelo é estatisticamente melhor que um modelo onde não temos uma variável explicativa, ou seja, melhor que apenas usando a média dos valores conforme vimos anteriormente em nosso gráfico.

Observamos também que a cada -0.06823 de redução da potência (hp), temos uma economia de uma unidade de consumo (mpg).

A estatística T, também se mostra significante à 5% de significância para a variável (hp). Isto faz sentido, já que temos apenas esta variável em nosso modelo e já vimos que tínhamos modelo atraǘes da estatística F.

Usando a função **predict**().

Podemos utilizar a função **predict** para obter inferências do modelo criado ao invés do cálculo manual como fizemos anteriormente:

```{r}
df_previsao = tibble("hp" = 190)
predict(modelo_uni, newdata = df_previsao)
```

Visualizando a inferência feita pela função **predict**():

O gráfico abaixo, mostra nossa estimativa de consumo para um veículo de 190 hps como vimos anteriormente, porém agora com o resultado da função **predict**().

```{r}
# Média da variável mpg:
df |> summarise(media = round(mean(mpg), 1))

# Gráfico da estimativa do modelo e uma reta com a média da variável:
df |> 
  ggplot(aes(x = hp, y= mpg)) +
  geom_point() +
  geom_smooth(method = "lm", se = F)+
  geom_abline(intercept = 20.1, slope = 0)+
  geom_point(aes(x = 190, y = 20.1),color = "red", size = 3)+
  geom_text_repel(aes(label = name), size = 2, color = "darkgray")+
  geom_point(aes(x = 190, y = 17.13),color = "darkgreen", size = 3) 

```

Coeficiente de ajuste do modelo $R^2$

O coeficiente $R^2$ é também chamado de coeficiente de ajuste do modelo, ou seja, o quanto da variância da variável dependente pode ser explicada pela variável explicativa.

Utilizamos a função summary para obter seu valor. Também podemos utilizar a correlação de Pearson calculada anteriormente para calculá-lo, pois seu valor é a correlação de Pearson $R$ ao quadrado.

```{r}
#Obtendo o R2
summary(modelo_uni)$r.squared

#Validando o R2, extraindo a raiz, deve bater com a correlação anterior.
sqrt(summary(modelo_uni)$r.squared)
```

# Variável Explicativa Qualitativa

Até o momento, criamos um modelo, onde a variável explicativa (X), era quantitativa. Mas e quando temos uma variável explicativa (X) qualitativa.

Digamos que iremos tentar prever nossa variável dependente (mpg) através da variável de números de cilindros (cyl). Se seguirmos os passos vistos até aqui, faríamos algo como, verificar sua correlação e depois criar um modelo univariado. Vejamos o que poderia ocorrer:

## Visualizando as correlações (ERRADO)

A seguir, iremos analisar as correlações e criar um modelo linear de forma similar à que fizemos até aqui.

::: callout-warning
CUIDADO!!! Estamos fazendo este procedimento de forma INCORRETA para mostrar alguns pontos importantes logo adiante.
:::

```{r}
df |> dplyr::select(mpg, cyl) |>
DataExplorer::plot_correlation()
```

Olhando estas correlações, poderíamos entender que quanto menor o número de cilindros, mais econômico é o veículo. Esta afirmação não está necessariamente incorreta, mas vamos vizualizar as observações em termos de consumo (mpg) e número de cilindros (cyl) e depois criar um modelo.

```{r}
df |> 
  ggplot(aes(x = cyl, y = mpg)) +
  geom_point()+
  geom_smooth(method = "lm", se=F)
```

## Criando o modelo (ERRADO)

Vamos criar um modelo linear simples, com o que vimos até aqui:

```{r}
modelo_uni_errado <- lm(mpg ~ cyl, df)
summary (modelo_uni_errado)
```

Vemos que os testes F e T tem seus p-valores menores que 5% (portanto passam nos testes de significância estatísticas). Vemos também que o $R²$ nos diz que este modelo explica 73% da variância de nossa variável dependente.

Se seguirmos com a análise de nossos resultados do modelo, vemos que o valor do $\beta$ está em -2.87. Isto, em tese, deveria nos dizer que, com uma redução de 2.87 cilindros, teríamos uma melhoria no consumo de uma unidade, ou seja, uma milha por galão (mpg). Oppssss...estranho, não é mesmo?

Pode não ser tão evidente, mas o modelo criado é **incorreto**, pois a variável "cyl", apesar de em nosso dataset estar configurada como "double" (quatitativa), ela é apenas um "label" para definir o tipo de cilindro é o automóvel, portanto é qualitativa.

Quando temos uma variável **qualitativa**, não temos média ou outras estatísticas de variáveis quantitativas. No máximo, podemos montar uma **tabela frequência**. Veja abaixo como ficaria a tabela de frequência (absoluta e relativa) da variável "cyl"

```{r}
#Frequencia absoluta e relativa:
as_tibble(table(df$cyl), 
          .name_repair = "unique") |>
  bind_cols(
    enframe(prop.table(
    table(df$cyl)))
  ) |> dplyr::select(num_cilindros = name, freq_absoluta = n, freq_relativa=value) |> 
  mutate (freq_relativa = scales::percent(as.numeric(freq_relativa)))
  
```

Como nossa variável "cyl" na tabela, está como tipo double, a função lm(), está tratando seus valores numéricos, ou seja, as diferenças entre 4, 6 e 8 como se fosse uma variável quantitativa e isto está incorreto!!!

## Ponderação Arbitrária

Aqui vale uma pequena pausa para entendermos melhor o que está acontecendo e seus impactos no modelo. Sabemos que devemos mudar a variável "cyl" que está originalmente quantitativa (4,6 e 8) para qualitativa. Porém, é um procedimento **comum e incorreto** atribuir valores de forma arbitrária, sendo estes, 1, 2 e 3 ou 4, 6 e 8, etc. Estes números são apenas "labels" para representar categorias desta variável.

Veja as médias adequadas quando mudamos a variável "cyl" como **qualitativa**:

```{r}
df_cyl_medias <- df |> mutate (cyl = as_factor(cyl)) |>
  group_by(cyl) |> summarise(mpg_media = mean(mpg))
df_cyl_medias
```

Neste caso, sabemos que em média, um veículo de 6 cilindros, tem um consumo de 19.7, enquanto que o de 4 e 8, tem respectivamente consumos médios de 26.7 e 15.1. Se tivessemos atribuído valores arbitrários, por exemplo, 4, 6 e 8, teríamos uma diferença de 2 entre cada um dos tipos de cilindros, o que é bem diferente do que vemos aqui.

Por exemplo: Veículos de 8 cilindros com diferença de 4.6 para os de 6 cilindros e 11.6 para os de 4 cilindros.

### Visualizando as diferenças com ponderação arbitrária

Visualizando o modelo errado (com ponderação arbitrária de 1, 2 e 3 e as médias corretas (em vermelho):

```{r}
df |>
  ggplot(aes(x=cyl, y=mpg))+
  geom_point()+
  geom_text_repel(aes(label=name))+
  geom_smooth(method="lm", se=F)+
  geom_point(data = df_cyl_medias, aes(x=parse_number(levels(cyl)), y=mpg_media), color = "red")
```

Observe como seria a inclinação dos betas considerando a frenquência média de cada categoria na variável cyl:

```{r}
df |>
  ggplot(aes(x=as_factor(cyl), y=mpg))+
  geom_point()+
  geom_text_repel(aes(label=name))+
  geom_line(data = df_cyl_medias, aes(x=cyl, y=mpg_media,group =1), size =1.2,color = "blue")+
  geom_point(data = df_cyl_medias, aes(x=cyl, y=mpg_media), color = "red")
```

A discrepância entre os valores lineares trazedos por uma ponderação arbitrária e os respectivos valores quando utilizamos adequadamente a variável qualitativa pode trazer viéses muito maiores que neste simples exemplo, impactando de forma brutal a acurácia do modelo.

::: callout-note
É por estre motivo que não podemos fazer a penderação arbitrária de valores para variáveis categóricas. Existe um procedimento adequado para lidar com esta situação que veremos a seguir.
:::

## Ajuste das variáveis qualitativas

Para adequar devidamente variáveis explicativas (x) categóricas para utilizarmos em modelos OLS, devemos criar variáveis adicionais "dummies". Apesar da função **lm**() conseguir lidar com variáveis qualitativas ao utilizarmos fatores, faremos isto através da função **dummy_columns**() para detalhar o processo por trás deste método. Iremos, a seguir, criar um data frame com a variável "cyl" como **fator** ao invés de double. Depois iremos utilzar a função **dummy_collumns** do pacote fastDummies para criar as variáveis dummies com base nos níveis dos fatores presentes:

```{r}
df_fct <- df |> mutate (cyl = as_factor(cyl))
df_fct_dummy <- fastDummies::dummy_columns(df_fct, select_columns = "cyl",
                                   remove_selected_columns = T,
                                   remove_most_frequent_dummy = F,
                                   remove_first_dummy = T)
glimpse(df_fct_dummy)
```

Neste caso, ele atribui a existência (1) ou não existência (0) para cada categoria da variável quantitativa -1. No caso da variável cilindro, teríamos 3 variáveis, uma para cada categoria. Porém, como sempre uma variável é resultante da ausência das demais, o valor final, será sempre $(1-n\;dummies)$. Em geral, utilizamos a categoria que possui a maior frequência (8 cilindros) como categoria de referência (parâmetro remove_most_frequent_dummy), apenas para exemplificar, escolhemos remover a primeira categoria (4 cilindros) com o parâmetro remove_first_dummy.

Agora temos 2 novas variáveis dummys (cyl_6 e cyl_8), sendo que a categoria de referência (cyl_4) é incluída no alpha da equação.

Neste caso, nossa equação ficaria:

$\hat y = \alpha + \beta_1 *cyl\_6+\beta_2*cyl\_8$

Com isso, sempre que tivermos zeros para cyl_6 e cyl_8, automaticamente sabemos que a categoria correponde a cyl_4.

## Criando o modelo com dummy

Agora com a variável "cyl" devidamente "dummizada" podemos criar o modelo:

```{r}
modelo_uni_dummy <- lm(mpg ~ cyl_6 + cyl_8, df_fct_dummy)
summary (modelo_uni_dummy)
```

Apesar de termos feito o processo de dummys, saiba que a função lm() é inteligente o suficiente e já faz este processo quando recebe uma variável factor. Veja:

```{r}
#A função lm() entende quando uma variável é factor como qualitativa e cria as dummies.
modelo_uni_fct <- lm(mpg ~ cyl, df_fct)
summary (modelo_uni_fct)
```

Neste caso, vemos que a estatística F passa no teste à 5% de significância, ou seja, temos um modelo. Vemos também que às dummies para 6 e 8 cilindros também passam no teste.

Ao ler os coeficientes, devemos sempre comparar com a categoria de referência. Por exemplo, podemos dizer que automóveis com 8 cilindros consomem 11.56 milhas por galão que automóveis de 4 cilindros, considerando todas as demais variáveis idênticas.

```{r}
prev <- tribble(~cyl,
             factor(4, levels = c(4,6,8)), 
             factor(6, levels = c(4,6,8)),
             factor(8, levels = c(4,6,8)))

predict(modelo_uni_fct, prev)
```

# Regressão multivariada

Adicionando outra variável explicativa peso (wt).

## Correlações

Por se tratar de outra variável quantitativa, iremos utilizar a correlação de Pearson para entender como estas variáveis se correlacionam:

```{r}
#Desta vez, iremos utilzar a função Chart.correlation do pacote PerformanceAnalytics
PerformanceAnalytics::chart.Correlation(df[c("mpg", "hp", "wt")])
```

Assim como a potência (hp), em nosso exemplo inicial, vemos que há uma considerável correlação negativa com a variável peso (wt).

## Criando um modelo multivariado

```{r}
#Função lm para obter os coeficientes alpha e beta das duas variáveis (hp e wt)
modelo_multi_1<- lm(mpg ~ hp + wt, data = df)
modelo_multi_1
```

Através da função **summary**() podemos observar que tanto a estatística F (do modelo), quanto as duas variáveis (estatística T) passam com um grau de confiança de 95%:

```{r}
summary(modelo_multi_1)
```

## Comparando os modelos

Podemos utilizar o $R^2$ para comparar o coeficiente de ajuste dos modelos:

```{r}
summary(modelo_uni)$r.squared
summary(modelo_multi_1)$r.squared
```

Neste caso, observamos que adicionando a variável peso, conseguimos explicar 82% da variância da variável dependente de consumo (mpg) ao invés dos 60% quando tínhamos apenas a variável pontência (hp).

## Resíduos e Homocedasticidade

A regressão linear possui algumas premissas que devem ser observadas. Os gráficos a seguir, nos ajudam a entender se os resíduos seguem uma distribuição normal, se há outliers, e se temos homocedasticidade, ou seja, variância homogênia nos resíduos. Estas premissas são importantes para a acurácia de uma modelo de regressão linear.

```{r}
par(mfrow=c(2,2))

plot (modelo_multi_1)
```

Para validar a leitura dos gráficos anteriores, podemos utilizar os testes estatísticos a seguir:

**Normalidade dos resíduos**:

```{r}
# Teste sharpiro-francia (ou sharpiro-wilk para amostrar < 30)
shapiro.test(modelo_multi_1$residuals)
```

Como podemos observar, nossos resíduos não passam no teste de normalidade (p-value \<= 0.05). A seguir deixaremos os demais testes com o código, mas devemos fazer algo a respeito desta premissa não atendida.

**Outliers nos resíduos**:

```{r}
# Os resíduos PADRONIZADOS devem estar entre -3 e +3 e mediana perto de zero
summary(rstandard(modelo_multi_1))
```

No caso acima, não temos outliers nos resíduos

```{r}
# Independência dos resíduos. Não se aplicaria aqui, mas deixamos o código. Em geral, quando temos análise longitudinal (medidas repetidas), ex. Time series.
# Recomendação de 1 e 3. p-value > 0.05 os resíduos são independentes. Teste adequando quando os resíduos atendem a normalidade.

car::durbinWatsonTest(modelo_multi_1)

```

Homocedasticidade:

```{r}
#Teste de Breush-Pagan. Também tem premissa de normalidade nos resíduos.
#H0 existe homocedasticidade e H1 Não existe, ou sejá, tem Heterocedasticidade.
# Neste caso, p-valor > 0.05, portanto temos homocedasticidade.
lmtest::bptest(modelo_multi_1)
```

Como nossa premissa de normalidade dos resíduos não foi antendida, podemos tentar fazer um transformação não linear em nossa variável dependente.

## Transformação de Box-Cox

Ao fazer uma transformação na variável dependente através de uma **Transformação de Box-Cox**, podemos ter uma variação mais uniforme. Vamos testar, criando um modelo atraveś de uma nova variável que possui uma transformação de Box-Cox da variável mpg e iremos analizar seus resíduos.

```{r}
df2 <- df
#Transformação de Box-Cox
#Estimando o lambda de BoxCox 
lambda_BC <- car::powerTransform(df2$mpg)
lambda_BC 
#Adicionando na base de dados: -->
df2$mpg_bc <- (((df2$mpg ^ lambda_BC$lambda) - 1) / lambda_BC$lambda)

modelo_multi_2_bc <- lm(mpg_bc~ hp + wt, df2)

summary(modelo_multi_2_bc)
```

Agora vamos visualizar e testar a normalidade dos resíduos:

```{r}
par(mfrow=c(2,2))
plot (modelo_multi_2_bc)
```

Testes de Normalidade, Homocedasticidade e Outliers:

```{r}
# Teste sharpiro-francia (ou sharpiro-wilk para amostrar < 30)
sf.test(modelo_multi_2_bc$residuals)
```

Como podemos observar, nossos resíduos agora **passam** no teste de normalidade (p-value \> 0.05). .

**Outliers nos resíduos**:

```{r}
# Os resíduos PADRONIZADOS devem estar entre -3 e +3 e mediana perto de zero
summary(rstandard(modelo_multi_2_bc))
```

No caso acima, não temos outliers nos resíduos

```{r}
# Independência dos resíduos. Não se aplicaria aqui, mas deixamos o código. Em geral, quando temos análise longitudinal (medidas repetidas), ex. Time series.
# Recomendação de 1 e 3. p-value > 0.05 os resíduos são independentes. Teste adequando quando os resíduos atendem a normalidade.

car::durbinWatsonTest(modelo_multi_2_bc)

```

Homocedasticidade:

```{r}
#Teste de Breush-Pagan. Também tem premissa de normalidade nos resíduos.
#H0 existe homocedasticidade e H1 Não existe, ou sejá, tem Heterocedasticidade.
# Neste caso, p-valor > 0.05, portanto temos homocedasticidade.
lmtest::bptest(modelo_multi_2_bc)
```

Salvando os fitted values Box-Cox:

Como fizemos o modelo com os valores transformados, precisamos lembrar de aplicar a fórmula inversa para termos o valor de mpg adequado:

```{r}
df2$mpg_bc_fitted <- (((modelo_multi_2_bc$fitted.values*(lambda_BC$lambda))+
                                    1))^(1/(lambda_BC$lambda))
```

Vamos utilizar agora a função **predict**() para estimar nosso consumo com o novo modelo multivariado com a a transformação de Box-Cox para um veículo com 3000 libras de peso e 190 de potência:

```{r}
df_previsao_bc = tibble("hp" = 190, "wt" = 3.0)
predict(modelo_multi_2_bc, newdata = df_previsao_bc)
```

Veja que esta previsão é o valor transformado, portanto, para saber o valor correto, devemos fazer a operação inversa da transformação, ou seja, multiplicar pelo lambda e elevar a 1/lambda.

```{r}
((3.06644 * (lambda_BC$lambda)) + 1)^(1/(lambda_BC$lambda))
```

Colocando direto no código, temos:

```{r}
df_previsao_bc = tibble("hp" = 190, "wt" = 3.0)
previsao <- predict(modelo_multi_2_bc, newdata = df_previsao_bc)

((previsao * (lambda_BC$lambda)) + 1)^(1/(lambda_BC$lambda))
```

## Visualizando a inferência

Neste caso, como temos duas variáveis explicativas, iremos criar um gráfico 3D para visualizar o hiper-plano da regressão:

```{r}
plot_ly(df2, x= ~mpg, y=~hp, z=~wt) |> 
  add_markers(name = "Dados Treino") |> 
  add_markers(x = 18.83, y = 190, z = 3.0, 
              name = "Previsao") 
```

## Multi-colinearidade e Stepwise

Temos um importante ponto a ser discutido quando se trata de modelos multi-variados. Se trata da multi-colinearidade e também das significâncias das variáveis na presença de outras variáveis explicativas.

Como vimos anteriormente, nosso modelo multi-variado, utilzando as variáveis **hp** e **wt**, teve um melhor ajuste que nosso modelo univariado utilzando apenas a variável **hp**. Digamos que agora, desejamos introduzir uma nova variável por acreditarmos que ela pode melhorar ainda mais nosso modelo. Ao olharmos as correlações com a variável dependente "mpg", observamos que a variável deslocamento "disp" tem forte correlação negativa com a variável dependente.

```{r}
cor(df$mpg, df$disp)
```

Com isso, criamos um modelo simples, apenas com esta variável, e vemos que ela não é somente significativa, mas também explica 72% da variância de mpg (R2 = .72).

```{r}
summary(lm(mpg ~ disp, df))
```

Com isso, decidimos introduzi-la em nosso modelo multi-variado, juntamente com as variaveis hp e wt com a intenção de deixá-lo ainda melhor.

```{r}
modelo_multi_3_autocor <- lm(mpg ~hp+wt+disp, df)
summary(modelo_multi_3_autocor)
```

Depois de analisar seu resultado através da função **summary**(), vemos que ela não passa em nosso test T, com p-value de 0.92851.

Isto ocorre, pois há uma forte correlação entre variáveis explicativas, neste caso entre disp e wt e um pouco menos forte entre disp e hp. Veja:

```{r}
df |> dplyr::select (mpg, hp, wt, disp) |> correlation::correlation(method = "pearson") |> plot()
```

Devido à isto, a variável disp, se torna menos relevante na presença das demais.

A função ols_vif_tol do pacote olsrr nos ajuda a fazer este disgnóstico de multi-colinearidade através do fator de inflação de variância (VIF) que vai de 1 até +(infinito) ou Tolerância (Tolerance) que vai de 0 até 1:

```{r}
# Tol: quanto mais próximo de 1 melhor.
# VIF: quanto maior pior. 
olsrr::ols_vif_tol(modelo_multi_3_autocor)
```

Em modelos com muitas variáveis explicativas, pode ser bastante demandante fazer manualmente a troca de variáveis, pois como vimos, algumas que são significativamente relevante para o modelo, podem deixar de sê-lo na presença de outras.

Para isto, há um algoritmo, chamado **stepwise** que remove as variáveis que não atingem o nível de significância definido e também já elemina às variáveis com alto nível de multicolinearidade. Vejamos como poderíamos utilzar em nosso modelo que contém a variável "disp":

```{r}
modelo_multi_3_step <- step(modelo_multi_3_autocor, k = 3.841459)

#De onde vem o argumento k = 3.841459? Este é o valor do chi-quadrado para nível de significância de 5% a 1 grau de liberdade. Veja:

qchisq(p = 0.05, df = 1, lower.tail = F)
round(pchisq(3.841459, df = 1, lower.tail = F),7)
```

Veja que o procedimento **stepwise** remove a variável "disp" automaticamente do modelo, gerando um modelo sem a variável com multi-colinearidade.

## Tranformações nas variáveis explicativas

Em alguns casos, as variáveis explicativas, também podem ser transformadas de maneira a adequar melhor sua forma funcional. Por exemplo, podemos ter cenários, onde a variável $X$ pode ter uma forma quadrátiva ($X^2$), logartimica ($log(X)$), etc. Nestes casos, podemos lançar mão de estratégias de transformação também para as variáveis explicativas. Vejamos este caso:

Nossa variável hp, é tem sua cauda direita ligeriamente esticada:

```{r}
df |> ggplot(aes(x=hp))+
  geom_histogram(aes(y= after_stat(density)),
                 colour = 1, fill = "white") +
  geom_density() +
  labs(title = "Variável hp ANTES da transformação")
```

Se fizermos uma transformação logartimica, podemos deixá-la mais próxima de uma distribuição normal, veja:

```{r}
df |> ggplot(aes(x=log(hp)))+
  geom_histogram(aes(y= after_stat(density)),
                 colour = 1, fill = "white") +
  geom_density()+
  labs(title = "Variável hp DEPOIS da transformação")
```

Não entraremos em detalhes sobre estas transformações neste artigo, mas é importante saber que estas são técnicas muito importantes para a acurácia de modelos reais.

Vejamos como ficaria um novo modelo, se implementássemos a transformação da variável explicativa (hp) juntamente com o modelo multi-variado com transformação de box-cox:

```{r}
#Transformação Logn da variável hp.

df3 <- df
df3$hp <- log(df3$hp)
#Transformação de Box-Cox
#Estimando o lambda de BoxCox 
lambda_BC <- car::powerTransform(df3$mpg)
lambda_BC 
#Adicionando na base de dados: -->
df3$mpg_bc <- (((df3$mpg ^ lambda_BC$lambda) - 1) / lambda_BC$lambda)

modelo_multi_4_hp_log <- lm(mpg_bc~ hp + wt, df3)

summary(modelo_multi_4_hp_log)

df3$mpg_bc_fitted <- (((modelo_multi_4_hp_log$fitted.values*(lambda_BC$lambda))+
                                    1))^(1/(lambda_BC$lambda))
```

Observe que as variáveis explicativas se mantém relevantes e o $R^2$ ajustado foi maior que o modelo anterior.

## Comparando os modelos

Para comparar os modelos iremos utilzar o $R²$ ajustado:

```{r}
modelos <- list(uni_hp = modelo_uni, 
                uni_cyl = modelo_uni_fct, 
                multi_hp_wt = modelo_multi_1, 
                multi_hp_wt_bc = modelo_multi_2_bc,
                multi_hp_wt_step = modelo_multi_3_step,
                multi_hp_wt_log = modelo_multi_4_hp_log)
r2_ajustado <- map_dfr(modelos, ~ summary(.)$adj.r.squared) |> 
  pivot_longer(cols = everything(), names_to = "modelo", values_to = "R2_ajustado")
r2_ajustado
```

Visualizando os ajustes dos modelos:

```{r}
r2_ajustado |> 
  ggplot(aes(x=as_factor(modelo), y=R2_ajustado, fill = modelo))+
  geom_col()+
  geom_label(aes(label = round(R2_ajustado,2)), vjust = 0, show.legend = F)+
  scale_y_continuous(limits = c(0, 1))+
  labs(title = "R2 ajustados dos modelos",x="Modelo",y="R2 Ajustado")
```

Com isto, temos o modelo multi-variado com transformação de Box-Cox na variável dependente e transformação logartimica na variável explicativa hp com o maior coeficiente de ajuste.

### Modelo Final

Visualizando o modelo com maior R2 até aqui:

```{r}
# Ajustando os fatores e junto os fitted values do modelo:
df3_fct <- df3 |> mutate (cyl = as_factor(cyl),
                    vs = as_factor(vs),
                    am = as_factor(am),
                    gear = as_factor(gear),
                    carb = as_factor(carb))

df3_fct |> 
  pivot_longer(cols = c(mpg, mpg_bc_fitted),names_to = "tipo", values_to = "mpg") |> group_by(tipo) |> 
  ggplot(aes(x=name, y=mpg, fill=tipo))+
  geom_col(position =  position_identity(),
           alpha = 0.5)+
  theme(axis.text.x = element_text(angle = 90))+
  labs(title="Modelo Final Multivariado com \nTransformação de Box-Cox e\nTranformação (log) na variável explicativa hp")+
  coord_flip()
  
```

\
Como podemos observar, nosso modelo não quebra as premissas para este tipo de técnica.

Este modelo seria representado pela seguinte equação:

$mpg = 5.1418 +(-0.2912 *log(hp))+(-0.1952*wt)$

## Bônus

Para finalizar, deixaremos um código exemplo de como utilzar a função **stepAIC**()do pacote MASS, que realiza o algoritmo stepwise tentando encontrar o melhor modelo, baseado na métrica AIC (Akaike information criteria):

$AIC =2K-2ln(L)$ onde:

$K$ é o número de variáveis explicativas e $L$ é a estimativa de máxima verossimilhança.

```{r}
df_fct <- df |> mutate (cyl = as_factor(cyl),
                    vs = as_factor(vs),
                    am = as_factor(am),
                    gear = as_factor(gear),
                    carb = as_factor(carb))
mod.inicial <- lm(mpg ~ . - name, df_fct)
mod.nulo <- lm(mpg ~ 1, df_fct)

MASS::stepAIC(mod.inicial, scope = list(upper=mod.inicial,
                                        lower=mod.nulo),
              direction = "backward")
```
