---
title: "Regressão Linear"
execute: 
  warning: false
format:
  html:
    toc: true
    html-math-method: katex
    css: styles.css
---

```{r}
library(tidyverse) 
library(janitor)
library(nortest)
library(DataExplorer)
library(ggrepel)
library (plotly)
library(MASS)
library(rgl)
library(car)

```

# Base de Dados

```{r}
# Selecionar base mtcars e criar uma coluna com os nomes dos modelos
df <- mtcars |> 
  rownames_to_column(var = "name") |> as_tibble()

df
```

# Variável dependente
Iremos selecionar como variável dependente o consumo (mpg).

## Visualizando a variável dependente:

```{r}
df |> 
  ggplot(aes(x=mpg)) +
  geom_histogram(binwidth = 2) 
```

## Teste de Normalidade Shapiro-Francia
Veja que este **NÃO** é uma premissa para uso da técnica, isso é geralmente na validação dos resíduos conforme veremos mais adiante. Estamos apenas apresentando como fazer o teste e analisando a variável dependente.


```{r}
# Teste de normalidade Shapiro-Francia
# p-valor <= 0.5 é não-normal, ou seja, maior a variável é normal
sf.test(df$mpg)
```

# Regressão univariada

Com já temos nossa variável dependente o consumo (mpg), iremos definir como variável explicativa, a potência (hp).

## Correlação

Iremos inicialmente entender como a variável dependente (mpg) está relacionada com a variável explicativa (hp). Por se tratar de duas variáveis quantitativas, iremos utilizar a correlação de Pearson. 
Este é um coeficiente que informa o quão forte é esta correlação, variando de -1 até 1. Sendo:
- Negativa (-1) - Quanto **maior** a variável dependente **menor** a variável explicativa
- Neutra (0) - Não há correlação entre as variáveis
- Positiva (1) - Quanto **maior** a variável dependente **maior** a variável
 


```{r}
# Variável explicativa escolhida = hp

cor(df$mpg, df$hp)
DataExplorer::plot_correlation(df[c("mpg", "hp")])
```

Nesta caso temos uma considerável correlação negativa (-0.78), ou seja, quanto menor a potência, maior o consumo.

## Gráfico de dispersão:

Atraǘes do gráfico abaixo, podemos ter uma nocção visual do que representa a correlação de -0.78.
Veja que se traçarmos uma reta atráves dos pontos, quanto maior o valor de mpg, menor o valor de hp e vice versa.

```{r}
df |> 
  ggplot(aes(x = hp, y= mpg)) +
  geom_point() +
  geom_smooth(method = "lm", se = F)+
  geom_text_repel(aes(label = name), size = 2, color = "darkgray")
```

## Criando um modelo linear simples
No caso de uma regressão linear simples, temos a seguinte equação:

Função: 
$\hat{y} = \alpha + \beta * X1$

O código abaixo, através da função "lm" irá criar um modelo de regressão linear simples, estimando os parâmetros para a equação de nossa reta.
Não iremos cobrir os detalhes deste processo, mas há diversos métodos para encontrar estes coeficientes, como Mínimos Quadrados Ordinários (OLS), Máxima Verossimilhança (MLE), entre outros.

```{r}
#Função lm para obter os coeficientes alpha e beta
modelo_uni <- lm(mpg ~ hp, data = df)
modelo_uni
```

Onde, o $\hat{y}$ representa o valor previsto de nosso modelo, o $\alpha$ o intercepto da reta e o $\beta$ a inclinação da reta. $X1$ neste caso seria nossa variável explicativa (hp).

Neste caso, nosssa função ficaria:

$\hat{y} = (30.09886) + [(-0.06823) * X1]$

Ou seja, se quisermos prever o consumo (mpg) à partir apenas da variável explicativa potencia (hp), faríamos:

$(30.09886) + [(-0.06823) * hp]$

Por exemplo, de acordo com nosso modelo, para um veículo com 190 de potência, teremos:

$(30.09886) + [(-0.06823) * 190]$ $(30.09886) - 12.9637 = \textbf{17.13516}$

Ou seja, nosso modelo prevê um consumo de 17.13 milhas por galão se um veículo tiver 190 de potência.

Com o modelo criado anteriormente, podemos utilizar a função **summary**() para extrair algumas informações bem importantes. Vejamos:

```{r}
summary(modelo_uni)
```

Há diversos resultados a serem observados, dentre eles, podemos observar que a estatística F, com p-value menor que 0.05 basicamente nos indica que nosso modelo é estatisticamente melhor que um modelo onde não temos uma variável explicativa, ou seja, apenas usando a média dos valores.

Observamos também que a cada -0.06823 de redução da potência (hp), temos uma economia de uma unidade de consumo (mpg).
A estatística T, também se mostra significante à 5% de significância para a variável (hp). Isto faz sentido, já que temos apenas esta variável em nosso modelo e já vimos que tínhamos modelo atraǘes da estatística F.

## Visualizando a inferência:
O gráfico abaixo, mostra nossa estimativa de consumo para um veículo de 190 hps.
Note que se tivessemos utilizado apenas a média, nossa estimativa seria pior que o modelo que criamos, pois ela seria de 20.1

```{r}
# Média da variável mpg:
df |> summarise(media = round(mean(mpg), 1))

# Gráfico da estimativa do modelo e uma reta com a média da variável:
df |> 
  ggplot(aes(x = hp, y= mpg)) +
  geom_point() +
  geom_smooth(method = "lm", se = F)+
  geom_abline(intercept = 20.1, slope = 0)+
  geom_point(aes(x = 190, y = 20.1),color = "red", size = 3)+
  geom_text_repel(aes(label = name), size = 2, color = "darkgray")+
  geom_point(aes(x = 190, y = 17.13),color = "darkgreen", size = 3) 

```

Usando a função **predict**().

Podemos utilizar a função **predict** para obter inferências do modelo criado ao invés do cálculo manual como fizemos anteriormente:

```{r}
df_previsao = tibble("hp" = 190)
predict(modelo_uni, newdata = df_previsao)
```

Coeficiente de ajuste do modelo $R^2$

O coeficiente $R^2$ pode ser utilizado como o coeficiente de ajuste do modelo, ou seja, o quanto da variância da variável dependente pode se explicada pela variável explicativa.

Utilizamos a função summary para obter seu valor. Também podemos utilizar a correlação de Pearson calculada anteriormente para calculá-lo.

```{r}
#Obtendo o R2
summary(modelo_uni)$r.squared

#Validando o R2, extraindo a raiz, deve bater com a correlação anterior.
sqrt(summary(modelo_uni)$r.squared)
```

# Regressão multivariada

Adicionando outra variável explicativa peso (wt).

## Correlações

Por se tratar de outra variável quantitativa, iremos utilizar a correlação de Pearson para entender como estas variáveis se correlacionam:

```{r}
DataExplorer::plot_correlation(df[c("mpg", "hp", "wt")])
```
Assim como a potência (hp), vemos que há uma considerável correlação negativa com a variável peso (wt).

## Criando um modelo multivariado

```{r}
#Função lm para obter os coeficientes alpha e beta das duas variáveis (hp e wt)
modelo_multi_1<- lm(mpg ~ hp + wt, data = df)
modelo_multi_1
```

Através da função **summary**() podemos observar que tanto a estatística F (do modelo), quanto as duas variáveis (estatística T) passam com um grau de confiança de 95%:

```{r}
summary(modelo_multi_1)
```

## Comparando os modelos

Podemos utilizar o $R^2$ para comparar o coeficiente de ajuste dos modelos:

```{r}
summary(modelo_uni)$r.squared
summary(modelo_multi_1)$r.squared
```

Neste caso, observamos que adicionando a variável peso, conseguimos explicar 82% da variância da variável dependente de consumo (mpg) ao invés dos 60% quando tínhamos apenas a variável pontência (hp).

## Resíduos e Homocedasticidade

A regressão linear possui algumas premissas que devem ser observadas. Os gráficos a seguir, nos ajudam a entender se os resíduos seguem uma distribuição normal, se há outliers, e se temos homocedasticidade, ou seja, variância homogênia nos resíduos. Estas premissas são importantes para a acurácia de uma modelo de regressão linear. 

```{r}
par(mfrow=c(2,2))

plot (modelo_multi_1)
```
Para validar a leitura dos gráficos anteriores, podemos utilizar os testes estatísticos a seguir:

**Normalidade dos resíduos**:

```{r}
# Teste sharpiro-francia (ou sharpiro-wilk para amostrar < 30)
shapiro.test(modelo_multi_1$residuals)
```

Como podemos observar, nossos resíduos não passam no teste de normalidade (p-value <= 0.05). A seguir deixaremos os demais testes com o código, mas devemos fazer algo a respeito desta premissa não atendida.

**Outliers nos resíduos**:

```{r}
# Os resíduos PADRONIZADOS devem estar entre -3 e +3 e mediana perto de zero
summary(rstandard(modelo_multi_1))
```
No caso acima, não temos outliers nos resíduos

```{r}
# Independência dos resíduos. Não se aplicaria aqui, mas deixamos o código. Em geral, quando temos análise longitudinal (medidas repetidas), ex. Time series.
# Recomendação de 1 e 3. p-value > 0.05 os resíduos são independentes. Teste adequando quando os resíduos atendem a normalidade.

car::durbinWatsonTest(modelo_multi_1)

```

Homocedasticidade:

```{r}
#Teste de Breush-Pagan. Também tem premissa de normalidade nos resíduos.
#H0 existe homocedasticidade e H1 Não existe, ou sejá, tem Heterocedasticidade.
# Neste caso, p-valor > 0.05, portanto temos homocedasticidade.
lmtest::bptest(modelo_multi_1)
```

Como nossa premissa de normalidade dos resíduos não foi antendida, podemos tentar fazer um transformação não linear em nossa variável dependente.

## Transformação de Box-Cox

Ao fazer uma transformação na variável dependente através de uma **Transformação de Box-Cox**, podemos ter uma variação mais uniforme. Vamos testar, criando um modelo atraveś de uma nova variável que possui uma transformação de Box-Cox da variável mpg e iremos analizar seus resíduos.

```{r}
df2 <- df
#Transformação de Box-Cox
#Estimando o lambda de BoxCox 
lambda_BC <- car::powerTransform(df2$mpg)
lambda_BC 
#Adicionando na base de dados: -->
df2$mpg_bc <- (((df2$mpg ^ lambda_BC$lambda) - 1) / lambda_BC$lambda)

modelo_multi_2_bc <- lm(mpg_bc~ hp + wt, df2)

summary(modelo_multi_2_bc)
```
Agora vamos visualizar e testar a normalidade dos resíduos:

```{r}
par(mfrow=c(2,2))
plot (modelo_multi_2_bc)
```

Testes de Normalidade, Homocedasticidade e Outliers:

```{r}
# Teste sharpiro-francia (ou sharpiro-wilk para amostrar < 30)
sf.test(modelo_multi_2_bc$residuals)
```

Como podemos observar, nossos resíduos agora **passam** no teste de normalidade (p-value > 0.05). .

**Outliers nos resíduos**:

```{r}
# Os resíduos PADRONIZADOS devem estar entre -3 e +3 e mediana perto de zero
summary(rstandard(modelo_multi_2_bc))
```
No caso acima, não temos outliers nos resíduos

```{r}
# Independência dos resíduos. Não se aplicaria aqui, mas deixamos o código. Em geral, quando temos análise longitudinal (medidas repetidas), ex. Time series.
# Recomendação de 1 e 3. p-value > 0.05 os resíduos são independentes. Teste adequando quando os resíduos atendem a normalidade.

car::durbinWatsonTest(modelo_multi_2_bc)

```

Homocedasticidade:

```{r}
#Teste de Breush-Pagan. Também tem premissa de normalidade nos resíduos.
#H0 existe homocedasticidade e H1 Não existe, ou sejá, tem Heterocedasticidade.
# Neste caso, p-valor > 0.05, portanto temos homocedasticidade.
lmtest::bptest(modelo_multi_2_bc)
```


## Salvando os fitted values Box-Cox:

Como fizemos o modelo com os valores transformados, precisamos lembrar de aplicar a fórmula inversa para termos o valor de mpg adequado:

```{r}
df2$mpg_bc_fitted <- (((modelo_multi_2_bc$fitted.values*(lambda_BC$lambda))+
                                    1))^(1/(lambda_BC$lambda))
```

Vamos utilizar agora a função **predict**() para estimar nosso consumo com o novo modelo multivariado com a a transformação de Box-Cox para um veículo com 3000 libras de peso e 190 de potência:

```{r}
df_previsao_bc = tibble("hp" = 190, "wt" = 3.0)
predict(modelo_multi_2_bc, newdata = df_previsao_bc)
```

Veja que esta previsão é o valor transformado, portanto, para saber o valor correto, devemos fazer a operação inversa da transformação, ou seja, multiplicar pelo lambda e elevar a 1/lambda. 

```{r}
((3.06644 * (lambda_BC$lambda)) + 1)^(1/(lambda_BC$lambda))
```

Colocando direto no código, temos:

```{r}
df_previsao_bc = tibble("hp" = 190, "wt" = 3.0)
previsao <- predict(modelo_multi_2_bc, newdata = df_previsao_bc)

((previsao * (lambda_BC$lambda)) + 1)^(1/(lambda_BC$lambda))
```


## Visualizando a inferência:

Neste caso, como temos duas variáveis explicativas, iremos criar um gráfico 3D para visualizar o hiper-plano da regressão:

```{r}
plot_ly(df2, x= ~mpg, y=~hp, z=~wt) |> 
  add_markers(name = "Dados Treino") |> 
  add_markers(x = 18.83, y = 190, z = 3.0, 
              name = "Previsao") 
```


:::callout-warning
Observe que até o momento, utilziamos variáveis explicativas quantitativas. Caso precise utilizar variáveis, você precisará fazer um processo transformando a variável qualitativa em variáveis "dummy".
:::

<!-- ## Visualizando as correlações (ERRADO) -->
<!-- A seguir, iremos analisar as correlações e criar um modelo linear de forma similar à que fizemos até aqui.  -->

<!-- :::callout-warning -->
<!-- CUIDADO!!! Estamos fazendo este procedimento de forma INCORRETA para mostrar alguns pontos importantes logo adiante. -->
<!-- ::: -->

<!-- ```{r} -->
<!-- df |> select(mpg, hp, cyl) |>  -->
<!-- DataExplorer::plot_correlation() -->
<!-- ``` -->

<!-- ```{r} -->
<!-- #fig <-  plot_ly(df, x = ~hp, y = ~mpg, z = ~cyl, color = ~mpg, colors = c('darkred', 'green'), size = 1) |>  -->
<!-- #  add_markers() -->

<!-- #fig -->
<!-- ``` -->

<!-- ## Criando o modelo (ERRADO) -->

<!-- Este modelo é errado, pois a variável "cyl", apesar de em nosso dataset estar configurada como "double" (quatitativa), ela é apenas um label para definir o tipo de cilindro é o automável, portanto é qualitativa. PNeste caso, suas proporções são: -->

<!-- ```{r} -->
<!-- #Frequencia absoluta: -->
<!-- table(df$cyl) -->
<!-- #Frequencia Relativa: -->
<!-- prop.table(table(df$cyl)) -->
<!-- ``` -->

<!-- Como na tabela df, ela está como double, a função lm(), está tratando seus valores numéricos, ou seja, as diferenças entre 4, 6 e 8. -->

<!-- ## Rodando o modelo ERRADO! -->

<!-- ```{r} -->

<!-- modelo_multi_errado <- lm(mpg ~ hp + cyl, df) -->
<!-- summary (modelo_multi_errado) -->

<!-- ``` -->

<!-- Aqui vale algumas observações. A primeira é que a variável "hp", deixa de ser estatisticamente significante (a 95% de confiança) após a introdução da variável "cyl". Veremos mais sobre este ponto a seguir. -->

<!-- Veja também que ele gera um beta para a variável "cyl". Sabemos que isto não significa nada, pois estamos ainda lidando com a variável cilindros como quantitativa, o que está incorreto! -->

<!-- ## Ponderação Arbitrária: -->

<!-- Para entender melhor, veja o que está acontencendo. Sabemos que devemos mudar a variável "cyl" que está originalmente quantitativa (4,6 e 8) para qualitativa. -->
<!-- Porém, é um procedimento comum e incorreto atribuir valores de forma arbitrária, sendo estes, 1,2 e 3 ou 4,6 e 8, etc. Estes números são apenas "labels" para representar categorias desta variável. -->

<!-- Veja as médias adequadas quando mudamos a variável "cyl" como qualitativa: -->

<!-- ```{r} -->
<!-- df_cyl_medias <- df |> mutate (cyl = as_factor(cyl)) |>  -->
<!--   group_by(cyl) |> summarise(media = mean(mpg)) -->
<!-- df_cyl_medias -->
<!-- ``` -->

<!-- ### Visualizando as diferenças com ponderação arbitrária -->
<!-- Visualizando o modelo errado (com ponderação arbitrária de 1,2 e 3 e as médias certas (em vermelho): -->

<!-- ```{r} -->
<!-- df |>  -->
<!--   ggplot(aes(x=cyl, y=mpg))+ -->
<!--   geom_point()+ -->
<!--   geom_text_repel(aes(label=name))+ -->
<!--   geom_smooth(method="lm", se=F)+ -->
<!--   geom_point(data = df_cyl_medias, aes(x=parse_number(levels(cyl)), y=media), color = "red") -->
<!-- ``` -->

<!-- Observe como seria a inclinação dos betas considerando a frenquência média de cada categoria na variável cyl: -->

<!-- ```{r} -->
<!-- df |>  -->
<!--   ggplot(aes(x=as_factor(cyl), y=mpg))+ -->
<!--   geom_point()+ -->
<!--   geom_text_repel(aes(label=name))+ -->
<!--   geom_line(data = df_cyl_medias, aes(x=cyl, y=media,group =1), size =1.2,color = "blue")+ -->
<!--   geom_point(data = df_cyl_medias, aes(x=cyl, y=media), color = "red") -->
<!-- ``` -->

<!-- :::callout-note -->
<!-- É por estre motivo que não podemos fazer a penderação arbitrária de valores para variáveis categóricas. Existe um procedimento adequado para lidar com esta situação que veremos a seguir. -->
<!-- ::: -->

<!-- # Ajuste das variáveis qualitativas -->

<!-- Para adequar devidamente variáveis explicativas (x) categóricas para utilizarmos em modelos OLS, devemos criar variáveis adicionais "dummys". -->
<!-- Faremos isto através da função dummy_columns: -->

<!-- ```{r} -->
<!-- df_fct <- df |> mutate (cyl = as_factor(cyl)) -->
<!-- df_fct_dummy <- fastDummies::dummy_columns(df_fct, select_columns = "cyl", -->
<!--                                    remove_selected_columns = T, -->
<!--                                    remove_first_dummy = T) -->
<!-- df_fct_dummy  -->
<!-- ``` -->
<!-- Neste caso, ele atribui a existência (1) ou não existência (0) para cada categoria da variável quantitativa -1. Ou seja, no caso da variável cilindro, teremos 2 novas variáveis dummys (cyl_6 e cyl_8), sendo que a categoria de referência (cyl_4) é incluída no alpha da equação. -->

<!-- # Criando o modelo com as dummys -->

<!-- Agora com a variável "cyl" devidamente "dummizada" podemos rodar o modelo: -->

<!-- Rodando o modelo com dummy: -->

<!-- ```{r} -->
<!-- modelo_multi_dummy <- lm(mpg ~ hp + cyl_6 + cyl_8, df_fct_dummy) -->
<!-- summary (modelo_multi_dummy) -->

<!-- ``` -->
<!-- Apesar de termos feito o processo de dummys, saiba que a função lm() é inteligente o suficiente e já faz este processo quando recebe uma variável factor. Veja: -->

<!-- ```{r} -->
<!-- #OBS: Não precisamos montar as dummys de forma manual, pois a função já faz estes procedimento.  -->
<!-- modelo_multi_fct <- lm(mpg ~ hp + cyl, df_fct) -->
<!-- summary (modelo_multi_fct) -->
<!-- ``` -->
<!-- ## Variáveis Explicativas Estatisticamente Significantes -->

<!-- Observe que apesar de no modelo com dummy, termos um p-valor para a estatística F, a estatística T da variável "hp" não é mais estatisticamente significante à 95% de nível de confiança. Isto pode se dar por 3 fatores: -->

<!-- 1-) Esta variável sozinha não é estatisticamente significante para explicar a variável y. Já vimos que isto não é verdade, pois ela passa quando fazemos um modelo com ela sozinha (p-valor = 0.000000179): -->

<!-- ```{r} -->
<!-- summary(modelo_uni) -->
<!-- ``` -->

<!-- 2-) Quando há multicolinearidade entre ela e outra variável explicativa. -->
<!-- No exemplo abaixo, vemos uma forte correlação entre "cilindro" com "hp" -->

<!-- ```{r} -->
<!-- df_fct_dummy |>  -->
<!--   select (mpg, hp, cyl_6, cyl_8) |>  -->
<!--   correlation::correlation(method = "pearson") |>  plot() -->
<!-- ``` -->
<!-- Neste caso, ao adicionar a variável cilindros (cyl), a variável "hp" deve ser removida do modelo, pois não é mais estatisticamente significante à 95% de confiança. -->

<!-- ## Stepwise -->

<!-- Poderíamos remover manualmente a variável "hp", porém em alguns casos, a troca de variáveis explicativas, faz com que níveis de significâncias se alterem dependendo da combinação, pois eventualmente, uma variável pode ser proxy de outra, etc. -->
<!-- Neste caso, podemos utilizar um algoritmo "stepwise" a um nível de significância de 5% para que tenhamos as melhores combinações de variáveis para o modelo. -->

<!-- ```{r} -->
<!-- modelo_multi_dummy_step <- step(modelo_multi_dummy,  -->
<!--                                       k = qchisq(p = 0.05, df = 1, lower.tail = F)) -->

<!-- summary(modelo_multi_dummy_step) -->
<!-- ``` -->
<!-- ## Normalidade dos Resíduos -->

<!-- Para verificar a normalidade dos resíduos iremos utilizar novamente o teste shapiro-francia.  -->

<!-- ```{r} -->
<!-- sf.test(modelo_uni$residuals) -->
<!-- sf.test(modelo_multi_dummy$residuals) -->
<!-- sf.test(modelo_multi_dummy_step$residuals) -->
<!-- ``` -->

<!-- Neste caso, observamos que os resíduos no modelo univariado não passa no teste de normalidade, já nos modelos multivariados ele passa.  -->
<!-- Porém, há alguns cenários onde a variável dependente é não linear. Neste caso, temos uma terceira possibilidade que veremos a seguir. -->

<!-- Visualizando o modelo certo: -->
<!-- Vamos agora incluir os valores estimados em nossa tabela de treinamento: -->

<!-- ```{r} -->
<!-- #df_fct$fit_1 <- modelo_uni$fitted.values -->
<!-- #df_fct$fit_2 <- modelo_multi_dummy$fitted.values -->
<!-- df_fct$fit_3 <- modelo_multi_dummy_step$fitted.values -->

<!-- df_fct |>  -->
<!--   ggplot(aes(x=mpg, y=mpg))+ -->
<!--   geom_smooth(aes(y = mpg), method = "lm",  -->
<!--               color = "green", size = 1.05, -->
<!--               linetype = "longdash") + -->
<!--  # geom_point(aes(y=fit_1), color = "blue") + -->
<!--  # geom_point(aes(y=fit_2), color = "orange") + -->
<!--   geom_point(aes(y=fit_3), color = "darkgray")  -->

<!-- ``` -->
<!-- Apesar dos resíduos do modelo após o stepwise serem aderentes à normalidade, iremos seguir com o terceiro cenário, apenas para ilusrtar o procedimento de quando isso não tivesse acontecido. -->

<!-- 3-) Quando a variável Y apresenta uma forma não linear. -->

<!-- Quando a variável dependende apresentar uma forma funcional não linear nosso modelo pode apresentar resíduosque não atendem à uma distribuição.  -->
<!-- Quando isto acontece, podemos tentar fazer uma transformação (ex: box-cox) para ajustar o modelo. -->


<!-- Por exemplo: -->

<!-- ```{r} -->
<!-- #Estimando o lambda de BoxCox -->
<!-- lambda_BC <- car::powerTransform(df_fct$mpg) -->
<!-- lambda_BC -->

<!-- #Adicionando na base de dados: -->
<!-- df_fct_dummy$mpg_bc <- (((df_fct$mpg ^ lambda_BC$lambda) - 1) /  -->
<!--                                       lambda_BC$lambda) -->
<!-- ``` -->

<!-- Agora, rodamos um modelo à partir da variável normalizada -->

<!-- ```{r} -->
<!-- modelo_multi_dummy_bc <- lm(mpg_bc ~ hp + cyl_6 + cyl_8, data = df_fct_dummy) -->
<!-- summary(modelo_multi_dummy_bc) -->
<!-- ``` -->
<!-- Aplicamos o stepwise: -->

<!-- ```{r} -->
<!-- modelo_multi_dummy_bc_step <- step(modelo_multi_dummy_bc,k = 3.841459) -->
<!-- summary(modelo_multi_dummy_bc_step) -->
<!-- ``` -->
<!-- Agora, podemos validar os resíduos à normalidade: -->

<!-- ```{r} -->
<!-- sf.test(modelo_multi_dummy_bc_step$residuals) -->
<!-- ``` -->

<!-- Adicoinamos os fitted na base e comparando os residuos: -->

<!-- ```{r} -->
<!-- df_fct_dummy$fit <- modelo_multi_dummy_bc_step$fitted.values -->
<!-- df_fct_dummy$residuos <- modelo_multi_dummy_bc_step$residuals -->

<!--   df_fct_dummy |>  -->
<!--   ggplot(aes(x = residuos)) + -->
<!--   geom_histogram(aes(y = ..density..),  -->
<!--                  color = "grey50",  -->
<!--                  fill = "grey90",  -->
<!--                  bins = 30, -->
<!--                  alpha = 0.6) + -->
<!--   stat_function(fun = dnorm,  -->
<!--                 args = list(mean = mean(modelo_multi_dummy_bc_step$residuals), -->
<!--                             sd = sd(modelo_multi_dummy_bc_step$residuals)), -->
<!--                 aes(color = "Curva Normal Teórica"), -->
<!--                 size = 2) + -->
<!--   scale_color_manual("Legenda:", -->
<!--                      values = "#FDE725FF") + -->
<!--   labs(x = "Resíduos", -->
<!--        y = "Frequência") + -->
<!--   theme(panel.background = element_rect("white"), -->
<!--         panel.grid = element_line("grey95"), -->
<!--         panel.border = element_rect(NA), -->
<!--         legend.position = "bottom") -->
<!-- ``` -->
<!-- Salvando os fitted na base: -->

<!-- ```{r} -->
<!-- df_fct_dummy$fit_bc_step <- (((modelo_multi_dummy_bc_step$fitted.values*(lambda_BC$lambda))+ -->
<!--                                     1))^(1/(lambda_BC$lambda)) -->
<!-- ``` -->

<!-- Visualizando: -->

<!-- ```{r} -->
<!-- df_fct_dummy |>  -->
<!--   ggplot(aes(x=mpg, y=mpg))+ -->
<!--   geom_smooth(aes(y = mpg), method = "lm",  -->
<!--               color = "green", size = 1.05, -->
<!--               linetype = "longdash") + -->
<!--  # geom_point(aes(y=fit_1), color = "blue") + -->
<!--  # geom_point(aes(y=fit_2), color = "orange") + -->
<!--   geom_point(aes(y=fit_bc_step), color = "darkgray")  -->
<!-- ``` -->

<!-- Comparando os modelos uni e multi-variado com variável dummy: -->

<!-- ```{r} -->
<!-- summary(modelo_uni)$adj.r.squared -->
<!-- summary(modelo_multi_dummy_step)$adj.r.squared -->
<!-- summary(modelo_multi_dummy_bc_step)$adj.r.squared -->
<!-- ``` -->

<!-- Observamos que o R2 ajustado (para comparação de modelos) é bem maior quando dicionamos a variável cyl. -->

<!-- #Modelo multi-nivel -->

<!-- Agora veremos uma abordagem multi-nivel, onde os betas são estimados em relação à grupos que adicionam  -->

<!-- ```{r} -->
<!-- df_pred <- tibble(hp = 190, cyl_6 = 0, cyl_8 = 1)   -->


<!-- fit <- predict(modelo_multi_dummy_bc_step, df_pred) -->
<!-- df_pred <- df_pred |> mutate (fit = ((fit * lambda_BC$lambda) + 1)^(1/(lambda_BC$lambda))) -->

<!-- df_pred  -->

<!-- ``` -->

<!-- ```{r} -->
<!-- df_fct_dummy$cyl <- df_fct$cyl -->
<!-- df_fct_dummy |>  -->
<!--   ggplot(aes(x=cyl, y=mpg))+ -->
<!--   geom_point()+ -->
<!--   geom_text_repel(aes(label=name))+ -->
<!--   geom_jitter(aes(y=fit_bc_step), color = "blue") -->
<!-- ``` -->

<!-- # Teste de Heterocedasticidade: -->

<!-- ```{r} -->
<!-- #Diagnóstico de Heterocedasticidade para o Modelo Stepwise com Box-Cox -->
<!-- olsrr::ols_test_breusch_pagan(modelo_multi_dummy_bc_step) -->
<!-- ``` -->

<!-- # Analise multi-nível -->

<!-- TBD -->
