---
title: "Regressão Linear"
execute: 
  warning: false
format:
  html:
    toc: true
    html-math-method: katex
    css: styles.css
    df-print: paged
---

```{r}
library(tidyverse) 
library(janitor)
library(nortest)
library(DataExplorer)
library(ggrepel)
library (plotly)
library(MASS)
library(rgl)
library(car)

```

# Base de Dados
Para os próximos exemplos iremos utilizar a base de dados MTCARS que possui informações sobre 32 veículos de revista Motor Trends de 1974.

```{r}
# Selecionar base mtcars e criar uma coluna com os nomes dos modelos
df <- mtcars |> 
  rownames_to_column(var = "name") |> as_tibble()

df
```

# Variável dependente
Para nossos experimentos, iremos selecionar como variável dependente o consumo (mpg). Ou seja, tentaremos criar modelo preditivos que, com base em variáveis explicativas, tentarão prever o valor do consumo (mpg) de um automóvel.

## Visualizando a variável dependente
Apenas para começar, vamos dar uma breve olhada em um histograma da variável dependente.

```{r}
df |> 
  ggplot(aes(x=mpg)) +
  geom_histogram(binwidth = 2) 
```

## Teste de Normalidade Shapiro-Francia
Veja que este **NÃO** é uma premissa para uso da técnica de regressão linear, isso é geralmente na validação dos resíduos conforme veremos mais adiante. Estamos apenas apresentando como fazer o teste e analisando a variável dependente.


```{r}
# Teste de normalidade Shapiro-Francia
# p-valor <= 0.5 é não-normal, ou seja, maior a variável é normal
sf.test(df$mpg)
```
Neste caso, observamos que nossa variável dependente tem uma forma funcional normal.

# Regressão univariada

Como já temos nossa variável dependente o consumo (mpg), iremos agora definir como variável explicativa, a potência (hp).
A idéia é tentar criar um modelo preditivo em função desta variável explicativa. 

## Correlação

Iremos inicialmente entender como a variável dependente (mpg) está relacionada com a variável explicativa (hp). Por se tratar de duas variáveis quantitativas, iremos utilizar a correlação de **Pearson**. 
Este é um coeficiente que informa o quão forte é esta correlação, variando de -1 até 1. Sendo:
- Negativa (-1) - Quanto **maior** a variável dependente **menor** a variável explicativa
- Neutra (0) - Não há correlação entre as variáveis
- Positiva (1) - Quanto **maior** a variável dependente **maior** a variável
 
```{r}
# Variável explicativa escolhida = hp

cor(df$mpg, df$hp)
DataExplorer::plot_correlation(df[c("mpg", "hp")])
```

Nesta caso temos uma considerável correlação negativa (-0.78), ou seja, quanto menor a potência, maior o consumo.

## Gráfico de dispersão:

Através do gráfico abaixo, podemos ter uma intuição visual do que representa a correlação de -0.78.
Veja que se traçarmos uma reta atráves dos pontos, quanto maior o valor de mpg, menor o valor de hp e vice versa.

```{r}
df |> 
  ggplot(aes(x = hp, y= mpg)) +
  geom_point() +
  geom_smooth(method = "lm", se = F)+
  geom_text_repel(aes(label = name), size = 2, color = "darkgray")
```

## Criando um modelo linear simples
No caso de uma regressão linear simples, temos a seguinte equação:

Função: 
$\hat{y} = \alpha + \beta * X1$

O código abaixo, através da função "lm" irá criar um modelo de regressão linear simples, estimando os parâmetros para a equação de nossa reta. Neste caso, iremos utilizar o método "OLS" (Ordinary Least Square) para estimativa dos parâmetros desta equação. 
Não iremos cobrir os detalhes deste processo, mas há diversos métodos para encontrar estes coeficientes, como Mínimos Quadrados Ordinários (OLS), Máxima Verossimilhança (MLE), entre outros.

```{r}
#Função lm para obter os coeficientes alpha e beta
modelo_uni <- lm(mpg ~ hp, data = df)
modelo_uni
```

Onde, o $\hat{y}$ representa o valor previsto de nosso modelo, o $\alpha$ o intercepto da reta, ou seja, que valor teórico termos caso a variável explicativa fosse zero. Temos também o $\beta$ que é inclinação da reta, ou seja, o quanto da variável explicativa é impactada em uma unidade. $X1$ neste caso, é o valor da nossa variável explicativa (hp).

Olhando estes coeficientes, podemos dizer que a cada 0.07 reduzida no valor da potência, aumentamos em uma milha por galão nosso consumo (mpg=miles per galon). 


Neste caso, nossa função ficaria:

$\hat{y} = (30.09886) + [(-0.06823) * X1]$

Ou seja, se quisermos prever o consumo (mpg) à partir apenas da variável explicativa potencia (hp), faríamos:

$(30.09886) + [(-0.06823) * hp]$

Por exemplo, de acordo com nosso modelo, para um veículo com 190 de potência, teremos:

$(30.09886) + [(-0.06823) * 190]$ =
$(30.09886) - 12.9637 = \textbf{17.13516}$

Ou seja, nosso modelo prevê um consumo de **17.13** milhas por galão se um veículo tiver **190** de potência.

## Analizando o Modelo
Com o modelo criado anteriormente, podemos juntar nossas estimativas (fitted values) à base de dados originais e comparar nossos resultados:

```{r}
media_mpg = round(summarise(df, m = mean(mpg))[[1]],1)
mpg_previsto = 17.13
hp_previsao = 190

bind_cols(df, modelo_uni$fitted.values) |> 
  rename (fitted = last_col()) |> 
  dplyr::select (orig = mpg, fitted, hp) |>  
  pivot_longer(cols = c("fitted", "orig")) |> 
  dplyr::rename (categoria = name, mpg = value) |> 
  ggplot(aes(x = hp, y = mpg, color = categoria)) +
  geom_point(size = 3, alpha = 0.6) +
  geom_point(x = hp_previsao, y= mpg_previsto, size = 5, color = "red")+
  geom_vline(aes(xintercept = hp_previsao), 
                 linetype = "dashed", color = "darkred")+
  geom_text(aes(hp_previsao, mpg_previsto , label = paste("Previsto: ", mpg_previsto), vjust = -1, hjust = -0.5), color = "red")+
  geom_hline(aes(yintercept = media_mpg), 
                 linetype = "dashed", color = "navyblue")+
  geom_text(aes(270,media_mpg, label = paste("Média: ", media_mpg), vjust = -1), show.legend = FALSE)
  
```

Veja que se tivéssemos escolhido apenas uma **média** da variável dependente (mpg), faríamos uma previsão de consumo de **20.1**. Como utilizamos nosso modelo, nossa previsão mais acurada, prevendo um valor de **17.13**.

Com o modelo criado anteriormente, podemos também utilizar a função **summary**() para extrair algumas informações bem importantes. Vejamos:

```{r}
summary(modelo_uni)
```

Há diversos resultados a serem observados, dentre eles, podemos observar que a estatística F, com p-value menor que 0.05 basicamente nos indica que nosso modelo é estatisticamente melhor que um modelo onde não temos uma variável explicativa, ou seja, melhor que apenas usando a média dos valores conforme vimos anteriormente em nosso gráfico.

Observamos também que a cada -0.06823 de redução da potência (hp), temos uma economia de uma unidade de consumo (mpg).

A estatística T, também se mostra significante à 5% de significância para a variável (hp). Isto faz sentido, já que temos apenas esta variável em nosso modelo e já vimos que tínhamos modelo atraǘes da estatística F.


Usando a função **predict**().

Podemos utilizar a função **predict** para obter inferências do modelo criado ao invés do cálculo manual como fizemos anteriormente:

```{r}
df_previsao = tibble("hp" = 190)
predict(modelo_uni, newdata = df_previsao)
```


Visualizando a inferência feita pela função **predict**():

O gráfico abaixo, mostra nossa estimativa de consumo para um veículo de 190 hps como vimos anteriormente, porém agora com o resultado da função **predict**().


```{r}
# Média da variável mpg:
df |> summarise(media = round(mean(mpg), 1))

# Gráfico da estimativa do modelo e uma reta com a média da variável:
df |> 
  ggplot(aes(x = hp, y= mpg)) +
  geom_point() +
  geom_smooth(method = "lm", se = F)+
  geom_abline(intercept = 20.1, slope = 0)+
  geom_point(aes(x = 190, y = 20.1),color = "red", size = 3)+
  geom_text_repel(aes(label = name), size = 2, color = "darkgray")+
  geom_point(aes(x = 190, y = 17.13),color = "darkgreen", size = 3) 

```
Coeficiente de ajuste do modelo $R^2$

O coeficiente $R^2$ é também chamado de coeficiente de ajuste do modelo, ou seja, o quanto da variância da variável dependente pode ser explicada pela variável explicativa.

Utilizamos a função summary para obter seu valor. Também podemos utilizar a correlação de Pearson calculada anteriormente para calculá-lo, pois seu valor é a correlação de Pearson $R$ ao quadrado.

```{r}
#Obtendo o R2
summary(modelo_uni)$r.squared

#Validando o R2, extraindo a raiz, deve bater com a correlação anterior.
sqrt(summary(modelo_uni)$r.squared)
```

# Variável Explicativa **Quantitativa**
Até o momento, criamos um modelo, onde a variável explicativa (X), era quantitativa. Mas e quando temos uma variável explicativa (X) qualitativa.

Digamos que iremos tentar prever nossa variável dependente (mpg) através da variável de números de cilindros (cyl). 
Se seguirmos os passos vistos até aqui, faríamos algo como, verificar sua correlação e depois criar um modelo univariado. Vejamos o que poderia ocorrer:


## Visualizando as correlações (ERRADO)
A seguir, iremos analisar as correlações e criar um modelo linear de forma similar à que fizemos até aqui.

:::callout-warning
CUIDADO!!! Estamos fazendo este procedimento de forma INCORRETA para mostrar alguns pontos importantes logo adiante.
:::

```{r}
df |> dplyr::select(mpg, cyl) |>
DataExplorer::plot_correlation()
```
Olhando estas correlações, poderíamos entender que quanto menor o número de cilindros, mais econômico é o veículo. Esta afirmação não está necessariamente incorreta, mas vamos vizualizar as observações em termos de consumo (mpg) e número de cilindros (cyl) e depois criar um modelo.

```{r}
df |> 
  ggplot(aes(x = cyl, y = mpg)) +
  geom_point()+
  geom_smooth(method = "lm", se=F)
```

## Criando o modelo (ERRADO)
Vamos criar um modelo linear simples, com o que vimos até aqui:

```{r}
modelo_uni_errado <- lm(mpg ~ cyl, df)
summary (modelo_uni_errado)
```

Vemos que os testes F e T tem seus p-valores menores que 5% (portanto passam nos testes de significância estatísticas). Vemos também que o $R²$ nos diz que este modelo explica 73% da variância de nossa variável dependente.

Se seguirmos com a análise de nossos resultados do modelo, vemos que o valor do $\beta$ está em -2.87. Isto, em tese, deveria nos dizer que, com uma redução de 2.87 cilindros, teríamos uma melhoria no consumo de uma unidade, ou seja, uma milha por galão (mpg). Oppssss...estranho, não é mesmo?

Pode não ser tão evidente, mas o modelo criado é **incorreto**, pois a variável "cyl", apesar de em nosso dataset estar configurada como "double" (quatitativa), ela é apenas um "label" para definir o tipo de cilindro é o automóvel, portanto é qualitativa. 

Quando temos uma variável **qualitativa**, não temos média ou outras estatísticas de variáveis quantitativas. No máximo, podemos montar uma **tabela frequência**. Veja abaixo como ficaria a tabela de frequência (absoluta e relativa) da variável "cyl"

```{r}
#Frequencia absoluta e relativa:
as_tibble(table(df$cyl), 
          .name_repair = "unique") |>
  bind_cols(
    enframe(prop.table(
    table(df$cyl)))
  ) |> dplyr::select(num_cilindros = name, freq_absoluta = n, freq_relativa=value)
  
```

Como nossa variável "cyl" na tabela, está como tipo double, a função lm(), está tratando seus valores numéricos, ou seja, as diferenças entre 4, 6 e 8 como se fosse uma variável quantitativa e isto está incorreto!!!

## Ponderação Arbitrária:

Aqui vale uma pequena pausa para entendermos melhor o que está acontecendo e seus impactos no modelo. 
Sabemos que devemos mudar a variável "cyl" que está originalmente quantitativa (4,6 e 8) para qualitativa.
Porém, é um procedimento **comum e incorreto** atribuir valores de forma arbitrária, sendo estes, 1, 2 e 3 ou 4, 6 e 8, etc. 
Estes números são apenas "labels" para representar categorias desta variável.

Veja as médias adequadas quando mudamos a variável "cyl" como **qualitativa**:

```{r}
df_cyl_medias <- df |> mutate (cyl = as_factor(cyl)) |>
  group_by(cyl) |> summarise(mpg_media = mean(mpg))
df_cyl_medias
```

Neste caso, sabemos que em média, um veículo de 6 cilindros, tem um consumo de 19.7, enquanto que o de 4 e 8, tem respectivamente consumos médios de 26.7 e 15.1. Se tivessemos atribuído valores arbitrários, por exemplo, 4, 6 e 8, teríamos uma diferença de 2 entre cada um dos tipos de cilindros, o que é bem diferente do que vemos aqui.

Por exemplo: Veículos de 8 cilindros com diferença de 4.6 para os de 6 cilindros e 11.6 para os de 4 cilindros.


### Visualizando as diferenças com ponderação arbitrária
Visualizando o modelo errado (com ponderação arbitrária de 1, 2 e 3 e as médias corretas (em vermelho):

```{r}
df |>
  ggplot(aes(x=cyl, y=mpg))+
  geom_point()+
  geom_text_repel(aes(label=name))+
  geom_smooth(method="lm", se=F)+
  geom_point(data = df_cyl_medias, aes(x=parse_number(levels(cyl)), y=mpg_media), color = "red")
```

Observe como seria a inclinação dos betas considerando a frenquência média de cada categoria na variável cyl:

```{r}
df |>
  ggplot(aes(x=as_factor(cyl), y=mpg))+
  geom_point()+
  geom_text_repel(aes(label=name))+
  geom_line(data = df_cyl_medias, aes(x=cyl, y=mpg_media,group =1), size =1.2,color = "blue")+
  geom_point(data = df_cyl_medias, aes(x=cyl, y=mpg_media), color = "red")
```

:::callout-note
É por estre motivo que não podemos fazer a penderação arbitrária de valores para variáveis categóricas. Existe um procedimento adequado para lidar com esta situação que veremos a seguir.
:::

## Ajuste das variáveis qualitativas

Para adequar devidamente variáveis explicativas (x) categóricas para utilizarmos em modelos OLS, devemos criar variáveis adicionais "dummys".
Faremos isto através da função dummy_columns:

```{r}
df_fct <- df |> mutate (cyl = as_factor(cyl))
df_fct_dummy <- fastDummies::dummy_columns(df_fct, select_columns = "cyl",
                                   remove_selected_columns = T,
                                   remove_first_dummy = T)
df_fct_dummy
```
Neste caso, ele atribui a existência (1) ou não existência (0) para cada categoria da variável quantitativa -1. Ou seja, no caso da variável cilindro, teremos 2 novas variáveis dummys (cyl_6 e cyl_8), sendo que a categoria de referência (cyl_4) é incluída no alpha da equação.

# Criando o modelo com as dummys

Agora com a variável "cyl" devidamente "dummizada" podemos rodar o modelo:

Rodando o modelo com dummy:

```{r}
modelo_uni_dummy <- lm(mpg ~ cyl_6 + cyl_8, df_fct_dummy)
summary (modelo_uni_dummy)

```
Apesar de termos feito o processo de dummys, saiba que a função lm() é inteligente o suficiente e já faz este processo quando recebe uma variável factor. Veja:

```{r}
#OBS: Não precisamos montar as dummys de forma manual, pois a função já faz estes procedimento.
modelo_multi_fct <- lm(mpg ~ hp + cyl, df_fct)
summary (modelo_multi_fct)
```


# Regressão multivariada

Adicionando outra variável explicativa peso (wt).

## Correlações

Por se tratar de outra variável quantitativa, iremos utilizar a correlação de Pearson para entender como estas variáveis se correlacionam:

```{r}
DataExplorer::plot_correlation(df[c("mpg", "hp", "wt")])
```
Assim como a potência (hp), vemos que há uma considerável correlação negativa com a variável peso (wt).

## Criando um modelo multivariado

```{r}
#Função lm para obter os coeficientes alpha e beta das duas variáveis (hp e wt)
modelo_multi_1<- lm(mpg ~ hp + wt, data = df)
modelo_multi_1
```

Através da função **summary**() podemos observar que tanto a estatística F (do modelo), quanto as duas variáveis (estatística T) passam com um grau de confiança de 95%:

```{r}
summary(modelo_multi_1)
```

## Comparando os modelos

Podemos utilizar o $R^2$ para comparar o coeficiente de ajuste dos modelos:

```{r}
summary(modelo_uni)$r.squared
summary(modelo_multi_1)$r.squared
```

Neste caso, observamos que adicionando a variável peso, conseguimos explicar 82% da variância da variável dependente de consumo (mpg) ao invés dos 60% quando tínhamos apenas a variável pontência (hp).

## Resíduos e Homocedasticidade

A regressão linear possui algumas premissas que devem ser observadas. Os gráficos a seguir, nos ajudam a entender se os resíduos seguem uma distribuição normal, se há outliers, e se temos homocedasticidade, ou seja, variância homogênia nos resíduos. Estas premissas são importantes para a acurácia de uma modelo de regressão linear. 

```{r}
par(mfrow=c(2,2))

plot (modelo_multi_1)
```
Para validar a leitura dos gráficos anteriores, podemos utilizar os testes estatísticos a seguir:

**Normalidade dos resíduos**:

```{r}
# Teste sharpiro-francia (ou sharpiro-wilk para amostrar < 30)
shapiro.test(modelo_multi_1$residuals)
```

Como podemos observar, nossos resíduos não passam no teste de normalidade (p-value <= 0.05). A seguir deixaremos os demais testes com o código, mas devemos fazer algo a respeito desta premissa não atendida.

**Outliers nos resíduos**:

```{r}
# Os resíduos PADRONIZADOS devem estar entre -3 e +3 e mediana perto de zero
summary(rstandard(modelo_multi_1))
```
No caso acima, não temos outliers nos resíduos

```{r}
# Independência dos resíduos. Não se aplicaria aqui, mas deixamos o código. Em geral, quando temos análise longitudinal (medidas repetidas), ex. Time series.
# Recomendação de 1 e 3. p-value > 0.05 os resíduos são independentes. Teste adequando quando os resíduos atendem a normalidade.

car::durbinWatsonTest(modelo_multi_1)

```

Homocedasticidade:

```{r}
#Teste de Breush-Pagan. Também tem premissa de normalidade nos resíduos.
#H0 existe homocedasticidade e H1 Não existe, ou sejá, tem Heterocedasticidade.
# Neste caso, p-valor > 0.05, portanto temos homocedasticidade.
lmtest::bptest(modelo_multi_1)
```

Como nossa premissa de normalidade dos resíduos não foi antendida, podemos tentar fazer um transformação não linear em nossa variável dependente.

## Transformação de Box-Cox

Ao fazer uma transformação na variável dependente através de uma **Transformação de Box-Cox**, podemos ter uma variação mais uniforme. Vamos testar, criando um modelo atraveś de uma nova variável que possui uma transformação de Box-Cox da variável mpg e iremos analizar seus resíduos.

```{r}
df2 <- df
#Transformação de Box-Cox
#Estimando o lambda de BoxCox 
lambda_BC <- car::powerTransform(df2$mpg)
lambda_BC 
#Adicionando na base de dados: -->
df2$mpg_bc <- (((df2$mpg ^ lambda_BC$lambda) - 1) / lambda_BC$lambda)

modelo_multi_2_bc <- lm(mpg_bc~ hp + wt, df2)

summary(modelo_multi_2_bc)
```
Agora vamos visualizar e testar a normalidade dos resíduos:

```{r}
par(mfrow=c(2,2))
plot (modelo_multi_2_bc)
```

Testes de Normalidade, Homocedasticidade e Outliers:

```{r}
# Teste sharpiro-francia (ou sharpiro-wilk para amostrar < 30)
sf.test(modelo_multi_2_bc$residuals)
```

Como podemos observar, nossos resíduos agora **passam** no teste de normalidade (p-value > 0.05). .

**Outliers nos resíduos**:

```{r}
# Os resíduos PADRONIZADOS devem estar entre -3 e +3 e mediana perto de zero
summary(rstandard(modelo_multi_2_bc))
```
No caso acima, não temos outliers nos resíduos

```{r}
# Independência dos resíduos. Não se aplicaria aqui, mas deixamos o código. Em geral, quando temos análise longitudinal (medidas repetidas), ex. Time series.
# Recomendação de 1 e 3. p-value > 0.05 os resíduos são independentes. Teste adequando quando os resíduos atendem a normalidade.

car::durbinWatsonTest(modelo_multi_2_bc)

```

Homocedasticidade:

```{r}
#Teste de Breush-Pagan. Também tem premissa de normalidade nos resíduos.
#H0 existe homocedasticidade e H1 Não existe, ou sejá, tem Heterocedasticidade.
# Neste caso, p-valor > 0.05, portanto temos homocedasticidade.
lmtest::bptest(modelo_multi_2_bc)
```


## Salvando os fitted values Box-Cox:

Como fizemos o modelo com os valores transformados, precisamos lembrar de aplicar a fórmula inversa para termos o valor de mpg adequado:

```{r}
df2$mpg_bc_fitted <- (((modelo_multi_2_bc$fitted.values*(lambda_BC$lambda))+
                                    1))^(1/(lambda_BC$lambda))
```

Vamos utilizar agora a função **predict**() para estimar nosso consumo com o novo modelo multivariado com a a transformação de Box-Cox para um veículo com 3000 libras de peso e 190 de potência:

```{r}
df_previsao_bc = tibble("hp" = 190, "wt" = 3.0)
predict(modelo_multi_2_bc, newdata = df_previsao_bc)
```

Veja que esta previsão é o valor transformado, portanto, para saber o valor correto, devemos fazer a operação inversa da transformação, ou seja, multiplicar pelo lambda e elevar a 1/lambda. 

```{r}
((3.06644 * (lambda_BC$lambda)) + 1)^(1/(lambda_BC$lambda))
```

Colocando direto no código, temos:

```{r}
df_previsao_bc = tibble("hp" = 190, "wt" = 3.0)
previsao <- predict(modelo_multi_2_bc, newdata = df_previsao_bc)

((previsao * (lambda_BC$lambda)) + 1)^(1/(lambda_BC$lambda))
```


## Visualizando a inferência:

Neste caso, como temos duas variáveis explicativas, iremos criar um gráfico 3D para visualizar o hiper-plano da regressão:

```{r}
plot_ly(df2, x= ~mpg, y=~hp, z=~wt) |> 
  add_markers(name = "Dados Treino") |> 
  add_markers(x = 18.83, y = 190, z = 3.0, 
              name = "Previsao") 
```


:::callout-warning
Observe que até o momento, utilziamos variáveis explicativas quantitativas. Caso precise utilizar variáveis, você precisará fazer um processo transformando a variável qualitativa em variáveis "dummy".
:::


<!-- ## Variáveis Explicativas Estatisticamente Significantes -->

<!-- Observe que apesar de no modelo com dummy, termos um p-valor para a estatística F, a estatística T da variável "hp" não é mais estatisticamente significante à 95% de nível de confiança. Isto pode se dar por 3 fatores: -->

<!-- 1-) Esta variável sozinha não é estatisticamente significante para explicar a variável y. Já vimos que isto não é verdade, pois ela passa quando fazemos um modelo com ela sozinha (p-valor = 0.000000179): -->

<!-- ```{r} -->
<!-- summary(modelo_uni) -->
<!-- ``` -->

<!-- 2-) Quando há multicolinearidade entre ela e outra variável explicativa. -->
<!-- No exemplo abaixo, vemos uma forte correlação entre "cilindro" com "hp" -->

<!-- ```{r} -->
<!-- df_fct_dummy |>  -->
<!--   select (mpg, hp, cyl_6, cyl_8) |>  -->
<!--   correlation::correlation(method = "pearson") |>  plot() -->
<!-- ``` -->
<!-- Neste caso, ao adicionar a variável cilindros (cyl), a variável "hp" deve ser removida do modelo, pois não é mais estatisticamente significante à 95% de confiança. -->

<!-- ## Stepwise -->

<!-- Poderíamos remover manualmente a variável "hp", porém em alguns casos, a troca de variáveis explicativas, faz com que níveis de significâncias se alterem dependendo da combinação, pois eventualmente, uma variável pode ser proxy de outra, etc. -->
<!-- Neste caso, podemos utilizar um algoritmo "stepwise" a um nível de significância de 5% para que tenhamos as melhores combinações de variáveis para o modelo. -->

<!-- ```{r} -->
<!-- modelo_multi_dummy_step <- step(modelo_multi_dummy,  -->
<!--                                       k = qchisq(p = 0.05, df = 1, lower.tail = F)) -->

<!-- summary(modelo_multi_dummy_step) -->
<!-- ``` -->
<!-- ## Normalidade dos Resíduos -->

<!-- Para verificar a normalidade dos resíduos iremos utilizar novamente o teste shapiro-francia.  -->

<!-- ```{r} -->
<!-- sf.test(modelo_uni$residuals) -->
<!-- sf.test(modelo_multi_dummy$residuals) -->
<!-- sf.test(modelo_multi_dummy_step$residuals) -->
<!-- ``` -->

<!-- Neste caso, observamos que os resíduos no modelo univariado não passa no teste de normalidade, já nos modelos multivariados ele passa.  -->
<!-- Porém, há alguns cenários onde a variável dependente é não linear. Neste caso, temos uma terceira possibilidade que veremos a seguir. -->

<!-- Visualizando o modelo certo: -->
<!-- Vamos agora incluir os valores estimados em nossa tabela de treinamento: -->

<!-- ```{r} -->
<!-- #df_fct$fit_1 <- modelo_uni$fitted.values -->
<!-- #df_fct$fit_2 <- modelo_multi_dummy$fitted.values -->
<!-- df_fct$fit_3 <- modelo_multi_dummy_step$fitted.values -->

<!-- df_fct |>  -->
<!--   ggplot(aes(x=mpg, y=mpg))+ -->
<!--   geom_smooth(aes(y = mpg), method = "lm",  -->
<!--               color = "green", size = 1.05, -->
<!--               linetype = "longdash") + -->
<!--  # geom_point(aes(y=fit_1), color = "blue") + -->
<!--  # geom_point(aes(y=fit_2), color = "orange") + -->
<!--   geom_point(aes(y=fit_3), color = "darkgray")  -->

<!-- ``` -->
<!-- Apesar dos resíduos do modelo após o stepwise serem aderentes à normalidade, iremos seguir com o terceiro cenário, apenas para ilusrtar o procedimento de quando isso não tivesse acontecido. -->

<!-- 3-) Quando a variável Y apresenta uma forma não linear. -->

<!-- Quando a variável dependende apresentar uma forma funcional não linear nosso modelo pode apresentar resíduosque não atendem à uma distribuição.  -->
<!-- Quando isto acontece, podemos tentar fazer uma transformação (ex: box-cox) para ajustar o modelo. -->


<!-- Por exemplo: -->

<!-- ```{r} -->
<!-- #Estimando o lambda de BoxCox -->
<!-- lambda_BC <- car::powerTransform(df_fct$mpg) -->
<!-- lambda_BC -->

<!-- #Adicionando na base de dados: -->
<!-- df_fct_dummy$mpg_bc <- (((df_fct$mpg ^ lambda_BC$lambda) - 1) /  -->
<!--                                       lambda_BC$lambda) -->
<!-- ``` -->

<!-- Agora, rodamos um modelo à partir da variável normalizada -->

<!-- ```{r} -->
<!-- modelo_multi_dummy_bc <- lm(mpg_bc ~ hp + cyl_6 + cyl_8, data = df_fct_dummy) -->
<!-- summary(modelo_multi_dummy_bc) -->
<!-- ``` -->
<!-- Aplicamos o stepwise: -->

<!-- ```{r} -->
<!-- modelo_multi_dummy_bc_step <- step(modelo_multi_dummy_bc,k = 3.841459) -->
<!-- summary(modelo_multi_dummy_bc_step) -->
<!-- ``` -->
<!-- Agora, podemos validar os resíduos à normalidade: -->

<!-- ```{r} -->
<!-- sf.test(modelo_multi_dummy_bc_step$residuals) -->
<!-- ``` -->

<!-- Adicoinamos os fitted na base e comparando os residuos: -->

<!-- ```{r} -->
<!-- df_fct_dummy$fit <- modelo_multi_dummy_bc_step$fitted.values -->
<!-- df_fct_dummy$residuos <- modelo_multi_dummy_bc_step$residuals -->

<!--   df_fct_dummy |>  -->
<!--   ggplot(aes(x = residuos)) + -->
<!--   geom_histogram(aes(y = ..density..),  -->
<!--                  color = "grey50",  -->
<!--                  fill = "grey90",  -->
<!--                  bins = 30, -->
<!--                  alpha = 0.6) + -->
<!--   stat_function(fun = dnorm,  -->
<!--                 args = list(mean = mean(modelo_multi_dummy_bc_step$residuals), -->
<!--                             sd = sd(modelo_multi_dummy_bc_step$residuals)), -->
<!--                 aes(color = "Curva Normal Teórica"), -->
<!--                 size = 2) + -->
<!--   scale_color_manual("Legenda:", -->
<!--                      values = "#FDE725FF") + -->
<!--   labs(x = "Resíduos", -->
<!--        y = "Frequência") + -->
<!--   theme(panel.background = element_rect("white"), -->
<!--         panel.grid = element_line("grey95"), -->
<!--         panel.border = element_rect(NA), -->
<!--         legend.position = "bottom") -->
<!-- ``` -->
<!-- Salvando os fitted na base: -->

<!-- ```{r} -->
<!-- df_fct_dummy$fit_bc_step <- (((modelo_multi_dummy_bc_step$fitted.values*(lambda_BC$lambda))+ -->
<!--                                     1))^(1/(lambda_BC$lambda)) -->
<!-- ``` -->

<!-- Visualizando: -->

<!-- ```{r} -->
<!-- df_fct_dummy |>  -->
<!--   ggplot(aes(x=mpg, y=mpg))+ -->
<!--   geom_smooth(aes(y = mpg), method = "lm",  -->
<!--               color = "green", size = 1.05, -->
<!--               linetype = "longdash") + -->
<!--  # geom_point(aes(y=fit_1), color = "blue") + -->
<!--  # geom_point(aes(y=fit_2), color = "orange") + -->
<!--   geom_point(aes(y=fit_bc_step), color = "darkgray")  -->
<!-- ``` -->

<!-- Comparando os modelos uni e multi-variado com variável dummy: -->

<!-- ```{r} -->
<!-- summary(modelo_uni)$adj.r.squared -->
<!-- summary(modelo_multi_dummy_step)$adj.r.squared -->
<!-- summary(modelo_multi_dummy_bc_step)$adj.r.squared -->
<!-- ``` -->

<!-- Observamos que o R2 ajustado (para comparação de modelos) é bem maior quando dicionamos a variável cyl. -->

<!-- #Modelo multi-nivel -->

<!-- Agora veremos uma abordagem multi-nivel, onde os betas são estimados em relação à grupos que adicionam  -->

<!-- ```{r} -->
<!-- df_pred <- tibble(hp = 190, cyl_6 = 0, cyl_8 = 1)   -->


<!-- fit <- predict(modelo_multi_dummy_bc_step, df_pred) -->
<!-- df_pred <- df_pred |> mutate (fit = ((fit * lambda_BC$lambda) + 1)^(1/(lambda_BC$lambda))) -->

<!-- df_pred  -->

<!-- ``` -->

<!-- ```{r} -->
<!-- df_fct_dummy$cyl <- df_fct$cyl -->
<!-- df_fct_dummy |>  -->
<!--   ggplot(aes(x=cyl, y=mpg))+ -->
<!--   geom_point()+ -->
<!--   geom_text_repel(aes(label=name))+ -->
<!--   geom_jitter(aes(y=fit_bc_step), color = "blue") -->
<!-- ``` -->

<!-- # Teste de Heterocedasticidade: -->

<!-- ```{r} -->
<!-- #Diagnóstico de Heterocedasticidade para o Modelo Stepwise com Box-Cox -->
<!-- olsrr::ols_test_breusch_pagan(modelo_multi_dummy_bc_step) -->
<!-- ``` -->

